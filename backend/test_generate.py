import os
from dotenv import load_dotenv
from transformers import pipeline
from huggingface_hub import login
import torch

# 1. Charger le .env
load_dotenv()
token = os.getenv("HF_TOKEN")

if not token:
    raise ValueError("‚ö†Ô∏è Token Hugging Face introuvable dans le .env")

# 2. Connexion √† Hugging Face
try:
    login(token=token)
    print("‚úÖ Connexion Hugging Face r√©ussie")
except Exception as e:
    print(f"‚ùå Erreur de connexion √† Hugging Face : {e}")
    exit(1)

# 3. Chargement du mod√®le
try:
    generator = pipeline(
        "text-generation",
        #model="mistralai/Mistral-7B-v0.1",
        model="bigscience/bloom-560m",
        torch_dtype=torch.float32,
        truncation=True, # <-- ici, dans la d√©finition du pipeline
        pad_token_id=2  # correspond souvent √† eos_token_id
    )
    print("‚úÖ Mod√®le charg√© avec succ√®s")
except Exception as e:
    print(f"‚ùå Erreur de chargement du mod√®le : {e}")
    exit(1)

# 4. Prompt de test
prompt = "Raconte-moi une blague courte en fran√ßais"

# 5. G√©n√©ration de texte
try:
    results = generator(
        prompt,
        max_length=60,
        num_return_sequences=1
    )
    print("üß† Texte g√©n√©r√© :")
    print(results[0]['generated_text'])
except Exception as e:
    print(f"‚ùå Erreur de g√©n√©ration : {e}")
